{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# II Autograd System\n",
    "## Tensors and Gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a tensor with gradient tracking\n",
    "x = torch.tensor([1.0, 2.0, 3.0], requires_grad=True)\n",
    "print(f'{x=}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing Gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform some operations\n",
    "y = x * x  # y = x^2\n",
    "z = y.mean()\n",
    "\n",
    "# Compute gradients\n",
    "z.backward()\n",
    "\n",
    "# Access the gradient of x\n",
    "print(f'{x=}, {x.grad=}')  # dz/dx = dy/dx evaluated at x = 2x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Computation and Backpropagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([2.0, 3.0, 4.0], requires_grad=True)\n",
    "y = x * x  # y = x^2\n",
    "z = y.sum()  # z = sum(y)\n",
    "\n",
    "# Compute gradients\n",
    "z.backward()\n",
    "print(f'{z=}, {z.grad=}\\n{y=}, {y.grad=}\\n{x=}, {x.grad=}\\n')  # Outputs the gradient of z with respect to x = 2x * 1, UserWarning\n",
    "\n",
    "# Calling backward again will accumulate the gradients\n",
    "y = x * x\n",
    "y.sum().backward()\n",
    "print(f'{z=}, {z.grad=}\\n{y=}, {y.grad=}\\n{x=}, {x.grad=}\\n')  # The gradients are accumulated, UserWarning\n",
    "\n",
    "# Reset\n",
    "x.grad.zero_()\n",
    "# z, y.grad.zero() - leafes only\n",
    "print(f'{z=}, {z.grad=}\\n{y=}, {y.grad=}\\n{x=}, {x.grad=}')  # UserWarning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chain Rule in Autograd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([1.0, 2.0, 3.0], requires_grad=True)\n",
    "y = x * x  # y = x^2\n",
    "z = y.sum()  # z = sum(y)\n",
    "\n",
    "z.backward()  # Compute gradients\n",
    "\n",
    "# For each element in x, the gradient will be 2*x, since y = x^2 and z = sum(y) = 2x * 1\n",
    "print(x.grad)  # Should print tensor([2.0, 4.0, 6.0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detaching Tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([1.0, 2.0, 3.0], requires_grad=True)\n",
    "y = x * x\n",
    "\n",
    "# Detach y from the computation graph\n",
    "y_detached = y.detach()\n",
    "\n",
    "# y_detached does not require gradients\n",
    "print(y_detached.grad)\n",
    "\n",
    "with torch.no_grad():\n",
    "    z = x * 2  # Inside this block, no operations will track gradients\n",
    "    print(z.requires_grad)  # Temporary False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([1.0, 2.0, 3.0], requires_grad=True)\n",
    "y = x * x\n",
    "z = y + y\n",
    "q = z.sum()\n",
    "\n",
    "# y.backward()  # Error, grad can be implicitly created only for scalar outputs\n",
    "q.backward()  # 2x * 1 + 2x * 1\n",
    "\n",
    "print(f'{q=}, {q.grad=}\\n{z=}, {z.grad=}\\n{y=}, {y.grad=}\\n{x=}, {x.grad=}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([1.0, 2.0, 3.0], requires_grad=True)\n",
    "y = x * x\n",
    "y1 = x * x\n",
    "y1 = y1.detach()\n",
    "z = y + y1\n",
    "q = z.sum()\n",
    "\n",
    "q.backward()  # 2x * 1 + (2x * 1 - detached)\n",
    "\n",
    "print(f'{q=}, {q.grad=}\\n{z=}, {z.grad=}\\n{y=}, {y.grad=}\\n{x=}, {x.grad=}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Autograd Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyReLU(torch.autograd.Function):\n",
    "    \"\"\"A custom operation with manually defined forward and backward passes.\"\"\"\n",
    "    @staticmethod\n",
    "    def forward(ctx, input):\n",
    "        ctx.save_for_backward(input)\n",
    "        return input.clamp(min=0)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        inpt, = ctx.saved_tensors\n",
    "        grad_input = grad_output.clone()\n",
    "        grad_input[inpt < 0] = 0\n",
    "        return grad_input\n",
    "\n",
    "x = torch.tensor([-1.0, 1.0, 2.0], requires_grad=True)\n",
    "relu = MyReLU.apply\n",
    "y = relu(x)\n",
    "y.backward(torch.ones_like(x))\n",
    "print(x.grad)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
